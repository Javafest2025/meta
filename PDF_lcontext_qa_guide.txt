Roadmap: Integrating Contextual Q&A into ScholarAI
Overview and Objectives

ScholarAI aims to let users ask questions about any uploaded PDF and receive accurate, context-based answers from a Gemini LLM API. This requires extracting the PDF’s content, retrieving relevant sections (Retrieval Augmented Generation or RAG), and using those as context for the AI. The solution will span frontend (React PDF viewer and chat UI) and backend (Java Spring Boot services with RabbitMQ and a Python extractor). Key goals include:

Seamless PDF Content Extraction: When a user opens a PDF, ensure its text (and figures/tables metadata) is extracted and stored if not done already. Use a background extractor service via RabbitMQ, with UI feedback (progress bar) during extraction.

Contextual Q&A Chat: Provide a chat interface in the PDF viewer where users can ask questions. The backend will fetch relevant content chunks from the extracted PDF JSON (instead of sending the entire document)
docs.scholarai.io
, then call the Gemini API with this context to get an answer. The AI’s responses should be grounded in the paper’s content, referencing sections or page numbers when appropriate.

Session-based Conversation: Maintain a conversation history for follow-up questions. Each chat session persists in the database so that follow-ups like “explain more” are understood in context of the previous Q&A. The conversation context (previous Q&A pairs and the PDF content) will be included in subsequent prompts as needed.

By implementing these, users can ask general questions (e.g. "Summarize this paper") or specific ones ("What is shown in Figure 2 on page 5?") and get accurate answers drawn from the PDF content.

Overall Workflow

PDF Viewer Load: When a user opens a PDF in the front-end viewer, the application checks if the PDF’s content has been extracted. This check is done via the project-service backend (e.g., an endpoint like GET /papers/{paperId}/extraction).

Trigger Extraction if Needed: If the PDF is not yet extracted, the front-end displays a progress indicator and triggers extraction. The project-service will send a message to a RabbitMQ queue (e.g., pdf_extract) with the PDF URL or ID. A Python Extractor service (FastAPI) listening on the queue will download the PDF (from B2 storage via the URL in the DB) and perform OCR/text extraction
docs.scholarai.io
. It extracts text sections, figures, tables, etc., into a structured JSON format and then returns the result (either by a RabbitMQ response queue or a callback API) to the project-service. The project-service stores this extracted JSON in the database (e.g., in a paper_content table).

Load Extracted Content: Once extraction is complete (the front-end can poll a status endpoint or get notified via WebSocket), the front-end stops the progress bar. The PDF viewer can now enable the Q&A chat interface. (The raw extracted content itself may not be directly shown to the user, but it’s loaded server-side for answering questions.)

User Asks a Question: The user opens the chat panel (embedded in the PDF viewer) and asks a question. The front-end (React) sends this question to the backend (e.g., POST /papers/{paperId}/chat), including a session identifier if a chat session is already active. If it’s the first question for this PDF, the backend will create a new ChatSession in the database and return a session ID.

Backend Retrieves Relevant Context: The PaperContextChatService (new backend service) receives the question. It fetches the extracted content JSON for the PDF from the DB. Instead of using the entire document (which could be very large), it implements chunked retrieval – selecting the most relevant sections or paragraphs from the paper to include as context
docs.scholarai.io
. For example, it might search for keywords from the question in the text or use embeddings to find similar sections. It will gather a handful of snippets (with identifiers like page numbers or section titles) that are most likely to contain the answer.

Gemini API Query: The service then constructs a prompt for the Gemini API. It may include a system message instructing the model to use the provided paper content only for answering. The prompt will incorporate the selected context snippets (each labeled with its source, e.g. “Page 5: …” or “Introduction: …”) and the user’s question. For example, the prompt could be: “You are a scholarly assistant. Use the following excerpts from the paper to answer the question. Ensure your answer is based on this content and cite page numbers. If the question is unclear, ask for clarification. Context: [relevant text snippets]. Question: [user’s question]. Answer:”
pragnakalp.com
. This guides Gemini to produce a fact-based answer using the paper’s content. The model parameters will be set for accuracy (e.g., relatively low temperature like 0.3-0.5 to minimize hallucinations
pragnakalp.com
).

AI Response: Gemini processes the prompt and returns an answer. The backend receives this answer text (via the Gemini API client or HTTP response). The PaperContextChatService then stores the Q&A pair in the database (as ChatMessage records linked to the session) and sends the answer back to the front-end.

Display Answer and Continue Chat: The front-end chat interface displays the answer beneath the user’s question. The user can ask follow-up questions in the same session. Each follow-up will repeat steps 5-7: the backend will include previous context (prior conversation and relevant paper snippets) so the AI understands references like “explain this further” or “what did you mean by X”. The chat history is maintained so that even if the user navigates away and back, the session can be resumed from the database.

Next, we break down the implementation details for each component and the modifications needed in specific files and services.

Frontend Implementation

The front-end is built with React 18 (Next.js) and Tailwind CSS. We will update the PDF viewer page to integrate the extraction workflow and the new Q&A chat interface. Key files to modify are PdfViewer.tsx (for handling PDF load and extraction) and ChatContainer.tsx (for the chat UI).

File: PdfViewer.tsx – PDF Loading & Extraction Flow

This component displays the PDF (using a viewer library) and will orchestrate the extraction process when the PDF is opened.

Check Extraction Status on Load: In the useEffect hook (when PdfViewer mounts or when the PDF ID changes), call the backend to check if the PDF’s content is extracted. For example, use fetch('/api/papers/{paperId}/extraction'). The backend (ExtractionService) will respond with a status and possibly extracted content availability.

If the response indicates the PDF is already extracted (status = “done”), we can proceed to enable Q&A features immediately.

If not extracted (status = “pending” or “not_found”), then:

Trigger extraction by calling a backend endpoint, e.g. POST /api/papers/{paperId}/extract. This will initiate the RabbitMQ flow on the server side.

Immediately show a progress bar or spinner in the UI to inform the user that the document is being processed. For example, render a <div> with a loading indicator on top of or below the PDF viewer.

Optionally, implement polling: every few seconds, call GET /api/papers/{paperId}/extraction to get updated status. The ExtractionService can return a field like "status": "in_progress" or "completed". Update the progress bar accordingly. (If the extractor service can report page-by-page progress, the backend could forward that, but initially a simple indeterminate spinner is fine.)

Once the status becomes "completed" (or the endpoint returns the extracted content), hide the progress bar. At this point, the PDF’s text content is ready in the database. You can enable the Q&A chat UI.

UI Integration: Ensure PdfViewer.tsx provides a place for the chat interface. For example, you might have a sidebar or a collapsible drawer in the PDF view page for the chat. This can be as simple as including a <ChatContainer paperId={currentPaperId} /> component in the JSX. Use Tailwind CSS to style it (e.g., a fixed panel on the right side of the viewer, or a bottom panel). If screen space is an issue, a toggle button (e.g., “Ask a question”) could show/hide the chat panel.

Edge Cases: If extraction fails (e.g., due to an unsupported PDF format or an error), show an error message to the user (perhaps “Failed to extract content. Please try again.”) and allow retrying the extraction by calling the endpoint again or prompt to contact support. Also handle timeouts by giving up after a certain time of polling and showing a message.

File: ChatContainer.tsx – Contextual Q&A Chat Interface

This component will manage the chat UI logic, showing the conversation and sending user queries to the backend. Key tasks and structure for this component:

State Management: Use React state (e.g., via useState or Redux, if in use) to maintain:

messages array (sequence of {role: 'user'|'assistant', text: string}) to render the chat history.

sessionId for the current chat session (obtained from backend). If null, it means a new session will be created on the first question.

inputQuestion bound to the chat textbox.

UI Layout: In the JSX, create a chat window UI:

A scrollable message list: iterate over messages state and display each message. Style user messages differently (e.g., align right, blue background) and assistant responses (align left, gray background) for clarity.

An input area with a <textarea> or <input> for the user’s question and a Send button. Also allow pressing Enter to submit.

Optionally, a button to “Reset Chat” to start a new session (clearing messages and sessionId).

Sending a Question: On clicking Send (or pressing Enter):

Capture the current inputQuestion text. If it's empty or just whitespace, ignore.

Append the user’s question to the messages state immediately (optimistically update UI). Set the role as 'user'. Clear the input box for usability.

Session Initialization: If sessionId is not yet set (first question on this PDF), call the backend to start a session and get an ID. For instance, a POST /api/papers/{paperId}/chat without a sessionId could return a new sessionId. Store that in state for subsequent calls. (Alternatively, the first question API response could include a sessionId field with the answer.)

API Call: Call the Q&A endpoint on the backend, e.g. POST /api/papers/{paperId}/chat/{sessionId}/ask. The payload includes the question text (and perhaps the sessionId again or it’s in URL). While this request is in progress, you can show a temporary message or spinner in the chat indicating “Assistant is typing…” or similar. For example, push a message { role: 'assistant', text: '…', pending: true } to the messages state to show an in-progress status.

Wait for the response. The backend will return the AI’s answer (text). Replace or append the assistant’s message in state with the actual answer text (e.g., update the pending message). Remove the loading indicator.

Continuous Conversation: The same sessionId is used for follow-up questions so that the backend knows the context. Each new user question is appended to the messages and sent to .../chat/{sessionId}/ask. The backend will handle including prior context. When the answer comes back, it’s added to messages. This way, the component state always reflects the entire conversation history in the UI.

UI Features:

Scroll the chat view to the bottom whenever a new message is added so the latest answer is visible.

Possibly allow the user to copy text from answers, since they might want to note citations or details.

If the user clicks “Reset Chat”, clear the messages and set sessionId to null (or call an API to explicitly start a fresh session). This allows them to ask unrelated questions in a new context if needed.

Tailwind Styling: Ensure the chat container has a contrasting background (e.g., light gray) against the PDF viewer. Messages can have rounded bubbles, etc., for readability. Keep the design simple and consistent with the rest of the app’s style.

With these front-end changes, the user experience will be: viewing the PDF with an adjacent chat panel, and an indicator when content is loading. Next, we detail the necessary backend implementation to support these features.

Backend Implementation

The backend uses Java Spring Boot (project-service). We will implement new services and endpoints in project-service to handle extraction and Q&A, and integrate with the existing database and the Gemini AI API. Additionally, we assume the existence of the Extractor microservice (FastAPI) which communicates via RabbitMQ, and a database (likely PostgreSQL or similar) where PDF metadata and extracted content are stored. Below are the breakdowns for key backend components:

Service: ExtractionService.java – Managing PDF Extraction

This Spring service (or component) will handle checking extraction status and triggering the asynchronous extraction process. Changes and tasks for this service include:

Check Existing Extraction: Implement a method like getExtractionStatus(paperId) that queries the database (perhaps via a JPA repository) to see if the PDF’s content is already extracted. For example, if there’s a PaperContent entity (with fields paperId and contentJson or similar), check for that record. If found, return status “completed” (and possibly the data or an ID to fetch it). If not found, return “not_extracted”. This method will be used when the front-end calls GET /papers/{id}/extraction.

Trigger New Extraction: If not already extracted, provide an endpoint or method startExtraction(paperId). This will:

Look up the PDF’s stored location (the B2 URL or file path) from the database (likely the paper’s record contains a URL in B2 storage).

Create a message payload containing the necessary info for extraction. This could be a JSON with fields like paperId, pdfUrl, maybe callbackQueue name, etc.

RabbitMQ Integration: Use Spring AMQP (RabbitTemplate or similar) to publish the message to the extractor queue (e.g., exchange “extractor” with routing key “extract.pdf”). The Python extractor will receive this, perform OCR extraction, and then send back the result.

Ensure RabbitMQ is configured in Spring (host, exchange, queue names set in application properties). Possibly define a message listener container to handle responses.

Mark the status as “in_progress”. You might insert a row in a separate ExtractionStatus table or update a flag in the paper’s record (like extraction_status = 'PENDING'). This way the getExtractionStatus can return “in_progress” on subsequent checks.

Handle Extraction Response: The extractor service, after processing, will need to return the extracted data. There are a couple of ways to handle this:

Easiest: The extractor could send a RabbitMQ message on a specific response queue (like extract.pdf.done). We can set up a listener in project-service for that queue. For example, use @RabbitListener(queues = "extract.pdf.done") on a method like handleExtractionResult(message). The message would contain paperId and the extracted content (probably compressed or a link if very large).

Alternative: The extractor could call a REST endpoint in project-service (like POST /api/papers/{id}/extractionResult) with the content. This requires the extractor to have HTTP access and the backend to expose such an endpoint. Either approach is fine; the RabbitMQ approach keeps the pipeline asynchronous.

In handleExtractionResult, parse the incoming data. Create a new PaperContent entity (if using JPA) with the paperId and the extracted JSON content (which may include sections, text, figures, etc.). Save it to the database. Mark extraction status as completed (maybe update the paper’s record or a status table).

Progress Updates (Optional): If we want real-time progress (like page-by-page extraction updates), we could have the extractor send intermediate messages (e.g., “page 5/12 done”). Handling those would be more complex; initially it’s sufficient to just know when it’s finished.

Extraction Data Structure: The extracted content will be structured JSON. For example, it might look like: { "title": "...", "authors": [...], "sections": [ { "heading": "1 Introduction", "text": "..." }, { "heading": "2 Methodology", "text": "..." } ], "figures": [ {"label": "Figure 1", "caption": "..."} ], "tables": [ ... ], "pages": {1: "text of page 1", ...} }. The exact schema depends on the extractor’s output. We should store this as a JSON column in the DB (PostgreSQL has JSONB type) or as text. This structured format will later allow retrieving specific parts (like figure captions or a certain page).

API Endpoints:

GET /papers/{paperId}/extraction: Returns a JSON with { status: "pending" | "completed", progress: X, content: null|somePreview }. If completed, it might not send the full content to the front (no need to send entire text to the browser), but it can signal readiness.

POST /papers/{paperId}/extract: Initiates extraction (calls startExtraction). This could return immediately with status “started” or the new status. The front-end will then poll the GET. Optionally, if the design prefers, the GET could itself trigger extraction when not found, but separating them is cleaner.

Citations: ScholarAI internally already uses an OCR-based system for PDF content extraction
docs.scholarai.io
. We follow that model: reading PDFs (including older scanned ones), isolating text and images. The known limitation is that figure labels might not be perfectly recognized
docs.scholarai.io
, but those details aside, once extraction is stored, the next phase is retrieval for Q&A.

Service: PaperContextChatService.java – Contextual Q&A and Chat Sessions

This is the core service coordinating the Q&A process. It handles retrieving the paper content, selecting relevant context, interacting with the Gemini API, and managing chat history. Implementation breakdown:

Chat Session Management:

Define a ChatSession entity (if not already). Fields: sessionId (UUID or long), paperId, userId (if users are authenticated; if not, session can be tied just to a client temp ID), createdAt, lastActive. A JPA repository ChatSessionRepository can provide CRUD operations.

Define a ChatMessage entity. Fields: id, sessionId (foreign key to ChatSession), role (USER or ASSISTANT), message (text content), timestamp. Repository ChatMessageRepository.

When a new question comes in without a session (the API call for first question), create a new ChatSession (persist it) and return its sessionId. The userId can be set from context (if the app has auth, else null or use some identifier if needed for multi-user support).

For each incoming question (whether new or follow-up), log it as a ChatMessage with role USER. After generating an answer, store the assistant’s response as another ChatMessage. This saves the full conversation in the DB.

Optionally, implement a method List<ChatMessage> getSessionHistory(sessionId) to fetch past messages if needed (e.g., to display past chat if user revisits). The front-end may or may not use this initially, but it's good to have for completeness.

Retrieving Extracted Content:

Use an ExtractionRepository (or reuse existing Paper repository if the extraction JSON is stored with the paper record) to fetch the extracted JSON for the given paperId. This JSON will be quite large for long papers, so we will selectively retrieve relevant parts for the question.

Implement a chunking strategy: break the document’s text into chunks that are easier to work with. Chunks could be the logical sections (if the JSON has a list of sections with text) or fixed-size blocks of text (e.g., 300-500 words each) or per page. For structured scholarly articles, using sections and subsections as chunks is effective (e.g., Introduction, Methods, Conclusion, etc.). Each chunk should carry an identifier (section title or page number). Storing precomputed chunks in DB might be useful, but on-the-fly is fine for now.

Relevance Ranking: Given the user’s question, we need to find which chunks are most likely to contain the answer:

Keyword Matching: A simple approach is to search for keywords from the question in the text. For example, split the question into tokens (after removing stopwords) and score each chunk by how many question tokens it contains or if it contains exact phrases. Rank the chunks by score.

Embedding-Based Retrieval (optional): For better accuracy, we could embed each chunk (using a smaller model or the Gemini embedding if available) and the question, then compute cosine similarity. This would require an embedding model or using Gemini’s embedding API, which might be complex to set up. In the short term, keyword search is easier to implement.

Metadata clues: If the question explicitly mentions “page 5” or “Figure 2” or “Conclusion”, we can directly pull those specific parts from the JSON (e.g., look up page 5 text, or find the figure with label “Figure 2” in the figures list). So the service should parse the question for such cues (perhaps via regex or simple contains check for “page” or “figure”). Those specific references should be prioritized in the context.

Select the top N chunks (perhaps 3 to 5 chunks) to include in the prompt. We want enough context to answer the question, but not exceed token limits. Gemini’s models often support very long contexts (some Gemini models claim up to 1000 pages PDFs natively
ai.google.dev
, but since we use text, we still must keep under model limits, e.g., a few thousand tokens).

Prompt Construction for Gemini:

Use the selected chunks to build a prompt string (or structured prompt if the API supports separate context vs question fields). Each chunk can be prefaced with a source note, e.g.:

Example:
Context:
Section: Abstract: “In this paper, we propose a new method for X...”.
Section: Results: “Experimentally, we found that the accuracy improved by 5%...”.
Question: “What is the main contribution of this paper?”
Answer: ...

Alternatively, format the prompt as the system message and user message in a chat format if Gemini API supports ChatML (OpenAI-compatible chat format). For instance:

System prompt: “You are a helpful academic assistant answering questions about a research paper. Use the provided paper excerpts to answer the question. Do not use outside knowledge. If you don’t find the answer in the text, say you don’t have that information. Always be concise and accurate.”

User message: Could include the context and question, for example: “Paper Excerpts:\n[1] (Page 5) ...text...\n[2] (Conclusion) ...text...\nQuestion: What do the results show about accuracy?”

Assistant message: (to be generated by the model).

Ensure the prompt clearly separates the context from the actual question so the model isn’t confused. The method used in a Vertex AI example is to explicitly label the context and question
pragnakalp.com
. We can do similarly.

Gemini Model Selection: Decide which Gemini model to use. Since we have document data, a Gemini with vision might not be necessary if we already extracted text. A model like gemini-1.0 or gemini-2.5 (if available) for text generation is fine. The prompt can just contain text. (If we wanted the model to also interpret images or figures directly, we could consider sending the PDF or images, but that’s out of scope with our text-based approach.)

Accuracy Settings: Use generation parameters to favor accuracy: e.g., temperature ~0.2-0.5, top_p ~0.3-0.5, top_k ~30-40 as needed. Lower temperature makes the model more deterministic and fact-focused
pragnakalp.com
. Also possibly limit the maximum tokens in the answer (e.g., if user asks for summary, maybe allow more tokens, but for a specific Q maybe a couple hundred tokens is enough).

Include an instruction for the model to refer to sections/pages in the answer if applicable. For example, “In your answer, indicate the source (page or section) of the information when relevant.” This could help make the answer more trustworthy.

Calling the Gemini API:

Use the Gemini API key (provided in backend config) to call the model. The Gemini API might be accessible via Google’s Vertex AI SDK or via a REST endpoint. For example, using Google’s GenAI SDK in Java, you would initialize a GenerativeModel with the Gemini model name (like "gemini-1.0"). Alternatively, call the Vertex API endpoint /v1beta3/projects/*/locations/*/publishers/google/models/gemini:generateText with an HTTP client.

Since the PDF is already processed, we are not using the direct PDF upload approach, but for reference: Gemini can take PDF bytes directly for summarization
ai.google.dev
. Our approach instead uses extracted text chunks for fine control.

Construct the request: if using Vertex SDK, something like:

model.generateText( 
     userInput,  // which contains our crafted prompt or messages 
     new GenerationOptions.Builder().temperature(0.4).build() 
);


If using raw HTTP, format the JSON payload accordingly.

Await the response from Gemini. If it’s streaming, handle the stream. But likely we’ll get the full answer in one response (the size is manageable if we limit max tokens).

Parse the response to extract the answer text. For example, the Vertex generateText might return an object with .getSafetyAttributes() and .getOutputText(). We just need the output text as the answer.

Return and Store Answer:

Take the answer text from Gemini and save it as a ChatMessage (role=ASSISTANT) in the DB with the current timestamp. Now the conversation history has the Q and A.

Return the answer in the HTTP response to the frontend. Wrap it in a JSON, e.g. { "answer": "...text..." }. If using a streaming response to the client, you could also consider Server-Sent Events or WebSockets, but to keep it simple, a normal HTTP response suffices (the front-end will show a spinner while waiting).

Handling Follow-up Questions:

If the user asks a follow-up in the same session, the service should include some of the conversation history in the prompt to maintain context. Specifically, Gemini (and most LLMs) won’t implicitly remember prior calls, so the history must be resent. We can include the last one or two Q&A pairs in the new prompt as additional context. For example:

System prompt can remain the same (“helpful assistant for the paper”).

In user prompt, before the new question, we might add: “Previous Q&A: Q: [user’s last question] A: [assistant’s last answer].” Then: “Follow-up question: [new question]?”.

This way, if the user says “Explain more about it”, the model sees what “it” refers to in the last answer
ibm.com
. This replicates conversational memory.

Alternatively, we could accumulate a running conversation prompt (like ChatGPT style). But be cautious with token length. Possibly summarize older parts if the conversation gets very long. For the initial implementation, focusing on the immediate previous exchange should handle things like pronoun references.

The service can detect a follow-up by session and perhaps by the question content (e.g., it lacks context or is shorter). Regardless, including the last Q&A is a safe approach for any question beyond the first.

Gemini Prompt Design Considerations

To maximize answer accuracy and relevance, we propose the following prompt design strategies (some of which are implemented in the steps above):

Explicit Role and Task Instruction: Clearly tell the model its role and to use the given context. E.g.: “You are a scholarly Q&A assistant. Answer the question using only the information from the provided paper excerpts. Do not include information not found in the excerpts.” This is similar to methods used in other PDF QA systems
pragnakalp.com
, framing the model as a search service analyzing provided text.

Provide Sufficient Context: Include the most relevant sections of the paper in the prompt. By chunking the paper and selecting the top sections, we ensure the model focuses on pertinent content
docs.scholarai.io
. Each chunk should be labeled (like “(Page 10)” or “(Methods section)”) so the model can reference them.

Ask the Question Clearly: After the context, state the user’s question distinctly (e.g., prefix with "Question:") so the model can differentiate between context and query
pragnakalp.com
.

Encourage Evidence in Answers: Instruct the model to refer to the source. For instance: “Where applicable, mention the page or section your answer was found in.” This can lead to answers like “The authors conclude that XYZ (see Conclusions, page 12).” This not only improves user trust but also keeps the model grounded.

Temperature and Length: Use a relatively low temperature to reduce creative fabrication and focus on factual accuracy
pragnakalp.com
. Limit the answer length if needed. For example, for a question about a specific detail, the answer should be a few sentences, not an essay. For a summary request, allow a higher token limit but still guide the style (maybe via an instruction like “Provide a concise summary.”).

Follow-up Handling: The prompt for follow-ups might include the previous Q&A as part of the context or a running dialogue. This helps the model handle context like pronouns or “elaborate further” queries. The design can mimic a conversation: e.g., User: “Summarize the method.” Assistant: “(provides summary)” User: “Can you explain more about how data was collected?” – The second query needs the model to know what “data” refers to, which it can infer if it sees the previous Q&A.

Fallback for Unknowns: Optionally, include a guideline: “If the answer is not in the provided content, respond that you don’t have that information from the paper.” This prevents the model from hallucinating. The example prompt from Vertex AI competition includes a similar line to ask user to rephrase if question is not understood
pragnakalp.com
. In our case, rephrasing might be less needed than indicating the info isn’t in the paper.

By refining the prompt with these points, we leverage best practices to improve answer correctness and context usage.

Storing and Retrieving Chat Sessions

The conversation persistence ensures a smooth user experience across turns and allows future retrieval of Q&A history. Implementation details:

Database Schema:

ChatSession Table: Columns: id (primary key, could be auto-generated or a UUID), paper_id (which PDF the session is tied to, foreign key to Papers table), user_id (foreign key to User, if applicable), created_at, last_active. Index on (user_id, paper_id) could be useful if you want at most one active session per user per paper, but we may allow multiple sessions per paper.

ChatMessage Table: Columns: id (PK), session_id (FK to ChatSession), role (enum or string: “USER” or “ASSISTANT”), content (text of the message), timestamp. Index on session_id for quick retrieval of a session’s messages.

Saving Messages: Every time the PaperContextChatService handles a question or gets an answer, create the corresponding ChatMessage entries. This can be done within the same method that calls Gemini: first save the user question (so we have it stored even before answer arrives), then after receiving answer, save the answer message. This way, even if something fails at the AI stage, we have a record of the question asked.

Retrieving History: If we want to display past chats when a user revisits a paper, we can add an API like GET /papers/{paperId}/chat/{sessionId} to fetch all messages. The front-end could call this when loading the chat UI if a session is ongoing. Initially, we might not need this if we always start a fresh session when the user opens the viewer (depending on requirements). But storing it anyway is good for future features (like showing recent Q&A or continuing where you left off).

Session Continuation: The front-end should store the sessionId (maybe in a React context or in local state while on that page). If the user closes the PDF and reopens it soon, you might decide to either continue the last session or start a new one. A good approach is to continue the last active session for that user-paper combination if it’s recent (to give a feeling of persistent chat). This logic can be implemented: e.g., GET /papers/{paperId}/chat could return an existing open sessionId or none. For simplicity, we can always start a new session on each visit unless future requirements say otherwise.

Multi-user Consideration: Chat sessions are per user. Ensure that one user cannot access another’s chat history. The backend should check the userId (from auth context) when returning session data. Only implement this if user accounts and auth are part of the system.

Cleanup: Over time, the chat_messages table might grow. We might implement retention policies (like deleting old sessions or limiting each session to X messages if storage is a concern). But given that PDF Q&A sessions will mostly be short-lived (user asks a few questions then stops), this may not be critical now.

Example Flow: User (ID 42) opens paper (ID 1001) and asks 3 questions in one session (ID 555). The DB will have one ChatSession row (id=555, user=42, paper=1001) and six ChatMessage rows (3 user questions, 3 assistant answers linked by session_id 555). If the user comes back tomorrow and starts new Q&A on the same PDF, a new ChatSession (id=556) is created, etc.

By implementing this, we preserve conversational context and can enhance the model’s understanding of follow-up queries (since we have the log of prior interactions to feed back in as needed).

File & Function-Level Implementation Guide

Finally, to tie the above pieces into actual code changes, here is a breakdown of modifications/additions in specific files and functions across the codebase:

Frontend (React/Next.js):

pages/pdf/[paperId].tsx or PdfViewer.tsx:

Import and include the ChatContainer component. Ensure the PDF viewer and ChatContainer can coexist (e.g., with CSS grid or flex layout).

In the component’s logic, on mount, call an API (via fetch or Axios) to GET /api/papers/{paperId}/extraction. Based on response, decide to trigger extraction. For triggering, call POST /api/papers/{paperId}/extract.

Manage a state like extractionStatus ('pending' | 'completed' | 'error') and possibly progress if available. If pending, render a <ProgressBar /> or a spinner overlay. If completed, render the ChatContainer.

After completion, perhaps store the extracted content (if we need it on front-end; likely not, as only backend uses it). The front-end just needs to know it’s ready.

components/ChatContainer.tsx:

Create state for messages, sessionId, and input text.

Render the chat messages list and input box as described.

Implement handleSend to post the question to backend:

const res = await fetch(`/api/papers/${paperId}/chat${sessionId ? '/' + sessionId : ''}`, {
   method: 'POST',
   headers: { 'Content-Type': 'application/json' },
   body: JSON.stringify({ question: inputQuestion, sessionId }) 
});
const data = await res.json();
// data should contain sessionId (for new sessions) and answer
if (!sessionId) setSessionId(data.sessionId);
addMessage({ role: 'assistant', text: data.answer });


(The exact endpoint path may vary; adjust based on how we implement in Spring).

Make sure to handle loading state: disable the input while waiting for response or show “typing…” as described.

Scroll management: use useEffect to scroll to bottom whenever messages state changes.

(If using Next.js API routes as proxy, ensure those routes forward to the Spring backend, or call the Spring service directly if CORS is open.)

Backend (Spring Boot):

ExtractionService.java:

Add @Autowired repository for PaperContent (extracted content).

Method public ExtractionStatus checkExtraction(String paperId): returns an enum or object indicating status. This will do DB lookup.

Method public void requestExtraction(String paperId): prepares RabbitMQ message. Use rabbitTemplate.convertAndSend(exchange, routingKey, message) to send. Message could be a simple Map or JSON string. Include paperId and possibly the B2 file URL (which you get from PaperRepository by paperId).

@RabbitListener (or similar) method public void handleExtractionResult(Message message): triggered when extractor publishes result. Parse message (e.g., JSON containing paperId and extractedContent string). Then:
PaperContent content = new PaperContent(paperId, extractedJson, ...); paperContentRepo.save(content);
Also update any status flags (maybe set a field in Paper entity like extracted=true).

Possibly notify waiting threads or send a WebSocket event. But simpler: the front-end polling will catch that on next poll.

Note: Ensure the RabbitMQ config (connection factory, queue names) is set up in Spring. Also manage error cases (if extraction fails or times out, maybe set a status = “error” in DB).

PaperContextChatService.java:

Autowire PaperContentRepository, ChatSessionRepository, ChatMessageRepository.

Method public ChatResponse askQuestion(String paperId, String sessionId, String question, User user): core logic for Q&A. Steps inside:

If sessionId is null: create new ChatSession (set paperId, userId), save to DB, get new sessionId.

Save new ChatMessage with role=USER, content=question.

Retrieve extracted content JSON for paperId (paperContentRepo.findByPaperId). If not found or not yet extracted, throw an error or return a response indicating not ready (the front-end should normally prevent reaching here until extracted).

Determine relevant snippets: implement a helper function like List<Snippet> findRelevantSnippets(String question, PaperContent content) that does keyword matching or simple retrieval. For each snippet (could be a small class with fields text and sourceInfo). For example, you might split content.text by paragraphs and for each paragraph that has any question keyword, mark it. Or if content JSON has sections, iterate sections.

Build the prompt string. Could use StringBuilder to concatenate a system instruction + context + question. If the Gemini API supports passing context separately (some APIs allow a list of messages), you might instead structure it accordingly.

Call Gemini API: implement a GeminiClient or use Vertex AI SDK. For a simple HTTP approach, you could use Spring’s RestTemplate or WebClient to POST to the Vertex endpoint. (Make sure to include the auth token or API key in headers). The response JSON will have the answer. Parse it.

Save a new ChatMessage with role=ASSISTANT, content=answer. Update session’s lastActive timestamp.

Build a ChatResponse object containing the answer (and sessionId if newly created) and return it.

Also possibly have method public ChatSession getOrCreateSession(String paperId, User user) to encapsulate session initialization. This could check if user already has a recent open session for that paper (depending on desired UX). For now, we create a new session for each unique chat instance.

Helper for follow-ups: If needed, fetch last one or two messages from ChatMessageRepository (where sessionId and role=ASSISTANT sorted by timestamp) to include the last answer in the prompt if the question is short or a known follow-up. This can be as simple as:

ChatMessage lastAnswer = chatMessageRepo.findTopBySessionIdAndRoleOrderByTimestampDesc(sessionId, Role.ASSISTANT);
if (lastAnswer != null && questionNeedsContext(question)) {
    prompt.append("Previous answer: ").append(lastAnswer.getContent()).append("\n");
}


where questionNeedsContext could check if question is like "explain more" or contains pronouns like "it/they/this".

Controller (perhaps PaperController.java or new ChatController.java):

Add endpoints:

GET /papers/{id}/extraction -> call ExtractionService.checkExtraction and return status/progress.

POST /papers/{id}/extract -> call ExtractionService.requestExtraction, return 202 Accepted or a status.

POST /papers/{id}/chat -> calls PaperContextChatService.askQuestion. The request body contains the question (and maybe sessionId, though sessionId could also be part of URL e.g. /chat/{sessionId}). Decide one style and implement accordingly. If sessionId is provided, use the existing session; if not, create new. Return JSON with the answer and sessionId.

Optionally, GET /papers/{id}/chat/{sessionId} to fetch past messages (if needed for front-end to load history).

Ensure to wire these endpoints with appropriate request DTOs. E.g., a ChatRequestDTO { String sessionId; String question; } and ChatResponseDTO { String sessionId; String answer; }.

The controller should also handle exceptions (like if extraction not ready, return 409 or 425 Too Early), and handle unauthorized access (if needed).

Extractor Microservice (FastAPI Python): (This is not the main focus, but mentioning for completeness)

It should have a RabbitMQ consumer for the extraction queue. When it gets a message, parse out the PDF URL, download the PDF (e.g., using requests.get or B2 SDK), run the extraction (perhaps using PDFPlumber, PyMuPDF, or an ML-based OCR as indicated by ScholarAI docs
docs.scholarai.io
).

Compose the JSON with text, figures, etc. Then either send it on a results queue or POST back.

If using RabbitMQ for result: use pika or similar to publish to the result queue with the paperId and content. If content is huge, compress or split the message (Rabbit can handle large messages but there might be limits; alternatively, upload the JSON to B2 or a DB and just send a pointer).

Ensure the extractor catches errors and reports failure (maybe send an error status message).

Conclusion and Next Steps

This roadmap provides a comprehensive plan to implement contextual PDF Q&A in ScholarAI. By dividing tasks across the frontend (React UI), backend (Spring Boot services), and the extractor service, we ensure a smooth integration. The approach follows a Retrieval-Augmented Generation (RAG) pattern
ibm.com
ibm.com
: extracting knowledge from documents and injecting it into the prompt for a large language model.

To implement: start with backend foundations (extraction pipeline and chat service) then connect the frontend UI. Thoroughly test each part:

Upload a PDF and verify extraction triggers and stores content.

Call the Q&A API with known questions (e.g., ask for the title, authors, a specific detail) to see if it fetches correct context and the model returns correct answers.

Test multi-turn dialogue (ask a follow-up) to ensure session context works.

Monitor the Gemini API usage and adjust chunk sizes or prompt if answers aren’t accurate initially.

By following this plan, ScholarAI users will be able to interact with research papers in a conversational way, getting quick answers grounded in the paper’s actual content. This adds significant value to the research experience, turning passive PDFs into interactive knowledge sources. With the system in place, further enhancements can include UI improvements, support for multiple PDFs at once (multi-document Q&A), or even highlighting answer sources in the PDF viewer. For now, the focus is on building a robust, accurate single-document Q&A system as outlined above.